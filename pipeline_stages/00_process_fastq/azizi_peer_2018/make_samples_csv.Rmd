---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 0.8.3
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
  language_info:
    codemirror_mode:
      name: ipython
      version: 3
    file_extension: .py
    mimetype: text/x-python
    name: python
    nbconvert_exporter: python
    pygments_lexer: ipython3
    version: 3.6.6
---

```{python}
import pandas as pd
import sys
sys.path.append("../../../lib/")
from jupytertools import * 
import numpy as np
import re
```

```{python}
run_table = pd.read_csv("SraRunTable.txt", sep="\t")
```

### Notes
* The samples are split-up (to reduce file size?): GSM3148586 consists of three files, the first to with excatecly 70,000,000 reads, the third with less. See also https://www.ncbi.nlm.nih.gov/sra/SRX4108637[accn]. 
* I should probably simply concatenate them. 
* The read-lengths are variable in some files. Not sure how STAR will deal with that. 
* Not sure what the expected number of cells per library is. The max in Supplementary table 1 is 3274. I will set it to arbitrarily large 10,000 and filter downstream in scanpy 

```{python}
display(run_table, ncol=200, n=5)
```

```{python}
run_table.columns
```

```{python}
prev_obs = pd.read_csv("../../../results/data_processed/azizi_peer_2018/obs.csv")
prev_obs
```

```{python}
map_origin = {
    "blood": "blood_peripheral",
    "breast": "normal_adjacent",
    "breast tumor": "tumor_primary",
    "lymph node": "lymph_node"
}
```

```{python}
samples = run_table[["Sample_Name", "source_name", "resident_tissue"]]\
                .drop_duplicates()\
                .rename({"Sample_Name": "samples", "source_name": "patient", "resident_tissue": "origin"}, axis="columns")\
                .assign(batch=1)\
                .assign(replicate=1)\
                .assign(platform="indrop_v2")\
                .assign(tumor_type="BRCA")\
                .assign(origin = lambda x : x["origin"].map(lambda k : map_origin[k]))\
                .assign(expected_cells=10000)\
                .assign(dataset="azizi_peer_2018")\
                .assign(read_length=101)\
                [["samples", "expected_cells", "read_length", "batch", "patient", "origin", "replicate", "platform", "tumor_type"]]
```

```{python}
samples
```

```{python}
samples.to_csv("./samples.csv")
```

```{python}

```
