---
params:
  input_file: NULL
  output_file: NULL
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 0.8.5
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
  language_info:
    codemirror_mode:
      name: ipython
      version: 3
    file_extension: .py
    mimetype: text/x-python
    name: python
    nbconvert_exporter: python
    pygments_lexer: ipython3
    version: 3.6.6
---

```{python}
# %load_ext autoreload
# %autoreload 2
import pandas as pd
import scanpy.api as sc
import numpy as np
import sys
sys.path.append("lib")
from jupytertools import setwd
from scio import concatenate
import os.path
import gc
setwd()
```

```{python}
INPUT_FILE = r.params['input_file']
OUT_FILE = r.params['output_file']
```

# Apply batch effect removal as in Schelker et al (2017)
This approach normalizes the expression of all cells to the expression of housekeeping genes.
Briefly outlined
* compute a scaling factor for each sample using the raw data

$$\frac{\text{mean of housekeeping genes of sample } i}{\text{mean of housekeeping genes across all samples}}$$


* apply the scaling factor to the scaled/normalized data

## Load files

```{python}
# Load housekeeping genes
hk_genes = pd.read_csv("tables/housekeeping_genes.txt", comment="#", sep="\t")
hk_genes.columns = ["gene_symbol", "NM"]
biomart = pd.read_csv("tables/biomart.tsv", sep="\t")

# map them to ENSG
ensg_hk = np.unique(
    biomart.join(hk_genes.set_index("gene_symbol"), on="HGNC symbol", how="inner")["Gene stable ID"].values)
```

```{python}
adata = sc.read(INPUT_FILE)
```

Following the procedure by Schelker, we need log-transformed TPM to start with. 
We need to 
 * undo the log transformation of `adata.raw`
 * normalize (will give us something like tpm)
 * redo the log-transformation. 

```{python}
adata_raw = sc.AnnData((np.e ** adata.raw.X.todense()) -1, obs=adata.obs, var=adata.raw.var)
sc.pp.filter_genes(adata_raw, min_cells=1)
sc.pp.scale(adata_raw)
```

## Compute HK score

```{python}
ensg_hk_fil = adata_raw.var_names[adata_raw.var_names.isin(ensg_hk)]
```

```{python}
adata.uns["hk_mean"] = np.mean(adata_raw[:, ensg_hk_fil].X).copy()
```

```{python}
adata.obs["hk_score"] = np.mean(adata_raw[:, ensg_hk_fil].X, axis=1).copy()
```

```{python}
adata.obs["hk_scaling_factor"] = adata.uns["hk_mean"]/adata.obs["hk_score"]
```

## Apply HK score

```{python}
tmp_x = adata.X * adata.obs["hk_scaling_factor"][:, np.newaxis]
```

```{python}
assert tmp_x.shape == adata.X.shape
```

```{python}
adata.X = tmp_x
```

```{python}
sc.pp.scale(adata)
```

```{python}
# Important! Do this on each batch-correct output file as the next step depends on it!
sc.tl.pca(adata, svd_solver='arpack')
sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)
sc.tl.umap(adata)
```

## Save results

```{python}
adata.write(OUT_FILE)
adata.write_csvs(os.path.basename(OUT_FILE))
```
